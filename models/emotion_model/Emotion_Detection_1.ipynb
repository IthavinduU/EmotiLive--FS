{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi1628/Emotion_Detection/blob/main/Emotion_Detection_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yCDtJZJcG6H3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.models import Sequential, Model, model_from_json\n",
        "from keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical  # Change here\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9LXQSLxG8xA",
        "outputId": "e8d141de-e9c9-484a-db45-454896a1118d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/emotional_dataset.csv\")\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oDZXRg7jYQep",
        "outputId": "9810f1e0-d9f4-4fbb-872f-725d7083b851"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CprWIfYKYhv9",
        "outputId": "a1cf0da2-5b00-4b83-8f2a-837859fdfc13"
      },
      "outputs": [],
      "source": [
        "data[\"Usage\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neFYC2SyZOW7"
      },
      "outputs": [],
      "source": [
        "# Convert \"Usage\" column to strings\n",
        "data[\"Usage\"] = data[\"Usage\"].astype(str)\n",
        "\n",
        "# Get unique values\n",
        "unique_values = np.unique(data[\"Usage\"].values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZmG6JWJZrvh"
      },
      "outputs": [],
      "source": [
        "# We just put the training samples into the train_data variable\n",
        "train_data = data [data.Usage == \"Training\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft-dvuiKZsBx"
      },
      "outputs": [],
      "source": [
        "# Assuming you have already split the pixel values\n",
        "train_pixels = train_data.pixels.str.split(\" \").tolist()\n",
        "\n",
        "# Creating DataFrame without specifying dtype\n",
        "train_pixels = pd.DataFrame(train_pixels)\n",
        "\n",
        "# Converting to numpy array\n",
        "train_images = train_pixels.values\n",
        "\n",
        "# If you need to convert to float explicitly\n",
        "train_images = train_images.astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCWtB1ffZsEw",
        "outputId": "5dfaad1c-73ce-4c99-ef11-e53bd416162b"
      },
      "outputs": [],
      "source": [
        "print(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJFMSLzyZsHw",
        "outputId": "6b8965eb-7d9b-438d-db1e-2c173787acb0"
      },
      "outputs": [],
      "source": [
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZrXuApJZ_ju"
      },
      "outputs": [],
      "source": [
        "# Let's define a function to display the image as 48x48 pixels\n",
        "def show(img):\n",
        "    show_image = img.reshape(48,48)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.imshow(show_image, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "aobwOqMtaFHA",
        "outputId": "e6b4a1a9-6435-4a13-b326-6310f99849a4"
      },
      "outputs": [],
      "source": [
        "# An example image from the training set\n",
        "show(train_images[750])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBgf8CoKaIFH",
        "outputId": "49afb0cb-0689-4bf9-b2de-6474ef1e7090"
      },
      "outputs": [],
      "source": [
        "# Let's see how many classes are in the training set\n",
        "\n",
        "train_labels_flat = train_data[\"emotion\"].values.ravel()\n",
        "train_labels_count = np.unique(train_labels_flat).shape[0]\n",
        "print('Number of different facial expressions: %d'%train_labels_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS_AD11TaQBM"
      },
      "outputs": [],
      "source": [
        "# Let's see the class of each data in the training set, that is,\n",
        "# the size of the training process, with One Hot.\n",
        "\n",
        "def dense_to_one_hot(labels_dense, num_classes):\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = np.arange(num_labels) * num_classes\n",
        "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "    return labels_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcwdQxzpaZWv",
        "outputId": "965d37dc-7638-4d18-8cf5-3a3706b95262"
      },
      "outputs": [],
      "source": [
        "y_train = dense_to_one_hot(train_labels_flat, train_labels_count)\n",
        "y_train = y_train.astype(np.uint8)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF_dQtRzad7l",
        "outputId": "15b3c383-dea6-4a2f-c171-561ba6effe34"
      },
      "outputs": [],
      "source": [
        "np.unique(data[\"Usage\"].values.ravel())\n",
        "print('Number of samples in the test dataset: %d'%(len(data[data.Usage == \"PublicTest\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV0kuDGdaiTa",
        "outputId": "da6f7fa9-c6fb-4785-ca7f-9b1308a96bb6"
      },
      "outputs": [],
      "source": [
        "test_data = data[data.Usage == \"PublicTest\"]\n",
        "\n",
        "# Assuming you have already split the pixel values\n",
        "test_pixels = test_data.pixels.str.split(\" \").tolist()\n",
        "\n",
        "# Creating DataFrame without specifying dtype\n",
        "test_pixels = pd.DataFrame(test_pixels)\n",
        "\n",
        "# Converting to numpy array\n",
        "test_images = test_pixels.values\n",
        "\n",
        "# If you need to convert to float explicitly\n",
        "test_images = test_images.astype(np.float64)\n",
        "\n",
        "print(test_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "irwyBWlPaq8p",
        "outputId": "53e4b652-694d-4bd5-ba3e-6cbe38bd1b1f"
      },
      "outputs": [],
      "source": [
        "# An example image from the training set\n",
        "show(test_images[150])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9PFjQaeatj4",
        "outputId": "6276ee3d-d081-4ca8-d94f-577dd5b869d2"
      },
      "outputs": [],
      "source": [
        "# Let's see the class of each data in the test set, that is,\n",
        "# the size of the training process, with One Hot.\n",
        "\n",
        "test_labels_flat = test_data[\"emotion\"].values.ravel()\n",
        "test_labels_count = np.unique(test_labels_flat).shape[0]\n",
        "\n",
        "y_test = dense_to_one_hot(test_labels_flat, test_labels_count)\n",
        "\n",
        "y_test = y_test.astype(np.uint8)\n",
        "\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "W8oP8B8gay8B",
        "outputId": "857d6018-0963-4e58-ec03-2f0a26f791ce"
      },
      "outputs": [],
      "source": [
        "plt.figure(0, figsize=(12,6))\n",
        "for i in range(1,13):\n",
        "  plt.subplot(3, 4, i)\n",
        "  plt.axis('off')\n",
        "\n",
        "  image = test_images[i].reshape(48,48)\n",
        "  plt.imshow(image, cmap='gray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDi9cISPa8Xx",
        "outputId": "be85a342-2d51-4d12-f8b3-8f223eb7d559"
      },
      "outputs": [],
      "source": [
        "# DEFINING A DEEP CONVOLUTIONARY NEURAL NETWORK MODEL\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#1. LAYER\n",
        "model.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#2. LAYER\n",
        "\n",
        "model.add(Conv2D(64, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2))\n",
        "model.add(Dropout(0.6)) #60% forgetting process (neuron deletion-dropout)\n",
        "\n",
        "### 3. LAYER\n",
        "model.add(Conv2D(32, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "### 4. LAYER\n",
        "model.add(Conv2D(32, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "### 5. LAYER\n",
        "model.add(Conv2D(32, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=2))\n",
        "model.add(Dropout(0.6)) #60% forgetting process (neuron deletion-dropout)\n",
        "\n",
        "### FULL CONNECTION LAYER\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.6)) #60% forgetting process (neuron deletion-dropout)\n",
        "\n",
        "### OUTPUT LAYER\n",
        "\n",
        "# Classification process (there are 7 emotion classes)\n",
        "model.add(Dense(7))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Determination of optimization and performance calculation metrics\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbCBNoMXa2Wd",
        "outputId": "2526057e-3b37-418b-ca94-14fb9edd8a56"
      },
      "outputs": [],
      "source": [
        "# Let's print the number of elements, height and width,\n",
        "# and number of channels information of the Training and Test sets on the screen.\n",
        "x_train = train_images.reshape(-1, 48, 48, 1)\n",
        "x_test = test_images.reshape(-1, 48, 48, 1)\n",
        "\n",
        "print('Train:', x_train.shape)\n",
        "print('Test:', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6oQRx7SbGfT",
        "outputId": "ab0af4da-dd55-47ff-dca1-9b5e7c3c8bda"
      },
      "outputs": [],
      "source": [
        "# Number of elements and emotion classes of Training and Test sets.\n",
        "\n",
        "print('Train:', y_train.shape)\n",
        "print('Test:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugfyNeCR7VIR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42mmWuSs7V_4",
        "outputId": "7a089796-4c38-4d4a-dc8b-b23214052a0a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Reduce batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Use data generator\n",
        "datagen = ImageDataGenerator()  # You can customize this based on your data augmentation needs\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Run the model\n",
        "checkpointer = ModelCheckpoint(filepath='/content/face_model.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "hist = model.fit(train_generator,\n",
        "                 steps_per_epoch=len(x_train) // batch_size,  # Adjust this based on your dataset size\n",
        "                 epochs=epochs,\n",
        "                 shuffle=True,\n",
        "                 validation_data=(x_test, y_test),\n",
        "                 callbacks=[checkpointer], verbose=2)\n",
        "\n",
        "# Save model to json\n",
        "model_json = model.to_json()\n",
        "with open('/content/face_model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Free up memory\n",
        "del x_train, y_train, x_test, y_test\n",
        "\n",
        "# Explicitly clear session and garbage collection\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "import gc\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBIjvTzyH92N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "0OAFC6BWbTI7",
        "outputId": "59399df7-9efb-4990-b9da-99b17d8cedf8"
      },
      "outputs": [],
      "source": [
        "# Process of expressing the Training and Validation results obtained\n",
        "# as a result of the training graphically and printing them on the screen\n",
        "\n",
        "plt.figure(figsize=(14,3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle('Eğitim', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.plot(hist.history['loss'], color ='r', label='Training Loss')\n",
        "plt.plot(hist.history['val_loss'], color ='b', label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.plot(hist.history['accuracy'], color ='g', label='Training Accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color ='m', label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kcWCGj_dbhcM",
        "outputId": "8dab4fd2-b3d2-4f31-e6a5-587631529510"
      },
      "outputs": [],
      "source": [
        "test = data[[\"emotion\", \"pixels\"]][data[\"Usage\"] == \"PrivateTest\"]\n",
        "test[\"pixels\"] = test[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
        "\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1sTTnIVbqYp",
        "outputId": "007554b3-61ec-4355-acb4-818a0861a11e"
      },
      "outputs": [],
      "source": [
        "x_test_private = np.vstack(test[\"pixels\"].values)\n",
        "y_test_private = np.array(test[\"emotion\"])\n",
        "x_test_private = x_test_private.reshape(-1, 48, 48, 1)\n",
        "y_test_private = to_categorical(y_test_private)\n",
        "x_test_private.shape, y_test_private.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8XFAY_lbrS5",
        "outputId": "b27a2a1f-1248-43d0-eab5-7c5192c27901"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(x_test_private, y_test_private, verbose=0)\n",
        "print(\"Accuracy status on Private Test:\", score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD0fortYr4Y7",
        "outputId": "7f9bbda7-6c1e-4227-9977-3dc0b9e77b22"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "model_best = load_model('/content/face_model.h5')\n",
        "\n",
        "# List of test image paths\n",
        "image_paths = [\"/content/Test_1.jpg\", \"/content/Test_2.jpg\", \"/content/Test_3.jpg\", \"/content/Test_4.jpg\"]\n",
        "\n",
        "# Process each test image\n",
        "results_list = []\n",
        "\n",
        "for image_path in image_paths:\n",
        "    # Load and preprocess the test image\n",
        "    test_image = image.load_img(image_path, target_size=(48, 48), grayscale=True)\n",
        "    test_data = image.img_to_array(test_image)\n",
        "    test_data = np.expand_dims(test_data, axis=0)\n",
        "    test_data = np.vstack([test_data])\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    results = model_best.predict(test_data, batch_size=1)\n",
        "\n",
        "    # Append the results to the list\n",
        "    results_list.append(results)\n",
        "\n",
        "# Convert the results list to a numpy array\n",
        "results_array = np.array(results_list)\n",
        "\n",
        "# Display the results\n",
        "print(results_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMAMgGEcsWTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w70wlmvrsW-M",
        "outputId": "d0b6d1a8-bc86-4284-f5e4-70e19c55f3df"
      },
      "outputs": [],
      "source": [
        "# Classes 7 emotional states\n",
        "class_names = ['Angry', 'Disgusted', 'Fear', 'Happy', 'Sad', 'Surprise', 'Natural']\n",
        "\n",
        "ind = 0.1 + 0.6 * np.arange(len(class_names))\n",
        "width = 0.4\n",
        "\n",
        "color_list = ['red', 'orangered', 'darkorange', 'limegreen', 'darkgreen', 'royalblue', 'navy']\n",
        "\n",
        "# List of test image paths\n",
        "image_paths = [\"/content/Test_1.jpg\", \"/content/Test_2.jpg\", \"/content/Test_3.jpg\", \"/content/Test_4.jpg\"]\n",
        "\n",
        "# Load the best model\n",
        "model_best = load_model('/content/face_model.h5')\n",
        "\n",
        "# Loop over each test image\n",
        "for image_path in image_paths:\n",
        "    # Load and preprocess the test image\n",
        "    test_image_original = image.load_img(image_path)\n",
        "    test_image = image.load_img(image_path, target_size=(48, 48), grayscale=True)\n",
        "    test_data = image.img_to_array(test_image)\n",
        "    test_data = np.expand_dims(test_data, axis=0)\n",
        "    test_data = np.vstack([test_data])\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    results = model_best.predict(test_data, batch_size=1)\n",
        "\n",
        "    # Display the original test image\n",
        "    plt.imshow(test_image_original)\n",
        "    plt.title('Entry Image', fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Color the results\n",
        "    for i in range(len(class_names)):\n",
        "        plt.bar(ind[i], results[0][i], width, color=color_list[i])\n",
        "\n",
        "    plt.title(\"Classification Results\", fontsize=20)\n",
        "    plt.xlabel(\"Facial Expressions Category\", fontsize=16)\n",
        "    plt.ylabel(\"Classification Score\", fontsize=16)\n",
        "    plt.xticks(ind, class_names, rotation=45, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Classification result with the highest rate:\", class_names[np.argmax(results)])\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between results of different images\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNEpsX1AR0vHcJU5UANUr2w",
      "include_colab_link": true,
      "mount_file_id": "1De16yRaaql5L0MJh5GxbHid9XGBJ2QEe",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
